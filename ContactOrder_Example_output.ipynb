{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bebe79f",
   "metadata": {},
   "source": [
    "## Sequence Entropy based on ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce59a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:01:21.130158Z",
     "iopub.status.busy": "2026-01-07T05:01:21.129964Z",
     "iopub.status.idle": "2026-01-07T05:01:24.088439Z",
     "shell.execute_reply": "2026-01-07T05:01:24.087480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU and set device\n",
    "# Python 3.14.0\n",
    "\n",
    "import esm\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize ESM-2 model (150M parameters) in evaluation mode\n",
    "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Define standard 20 amino acids vocabulary for filtering predictions\n",
    "aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "aa_to_idx = {aa: alphabet.get_idx(aa) for aa in aa_list}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a680034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:01:24.091107Z",
     "iopub.status.busy": "2026-01-07T05:01:24.090549Z",
     "iopub.status.idle": "2026-01-07T05:01:24.094737Z",
     "shell.execute_reply": "2026-01-07T05:01:24.094148Z"
    }
   },
   "outputs": [],
   "source": [
    "def rectify_logits(logits: torch.Tensor, epsilon: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes numerically stable log-probabilities from logits using epsilon smoothing.\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor of shape (vocab_size,).\n",
    "        epsilon: Floor value to prevent log(0).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (vocab_size,) containing log-probabilities in original dtype.\n",
    "    \"\"\"\n",
    "    # Use float64 to maximize precision during softmax accumulation\n",
    "    p = logits.to(dtype=torch.float64).softmax(dim=-1)\n",
    "    \n",
    "    # Apply smoothing and renormalize to ensure valid distribution (sum=1)\n",
    "    p = p + epsilon\n",
    "    p = p / p.sum(dim=-1, keepdims=True)\n",
    "    \n",
    "    return torch.log(p).to(dtype=logits.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab4631b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T05:01:24.096317Z",
     "iopub.status.busy": "2026-01-07T05:01:24.096160Z",
     "iopub.status.idle": "2026-01-07T05:01:27.533801Z",
     "shell.execute_reply": "2026-01-07T05:01:27.533335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sequence entropy is 3.564803\n"
     ]
    }
   ],
   "source": [
    "def compute_masked_entropy(seq: str):\n",
    "    \"\"\"\n",
    "    Computes per-position Shannon entropy over the 20 standard amino acids using MLM masking.\n",
    "    \n",
    "    The function iteratively masks each position, obtains the model's predicted distribution \n",
    "    over the 20 standard AAs, and calculates the entropy of this conditional distribution.\n",
    "\n",
    "    Args:\n",
    "        seq: Wild-type protein sequence (string of length L).\n",
    "\n",
    "    Returns:\n",
    "        entropies: list[float] of length L, Shannon entropy (in bits) at each position.\n",
    "    \"\"\"\n",
    "    L = len(seq)\n",
    "    mut_pref_matrix = np.zeros((L, 20), dtype=np.float32)\n",
    "    \n",
    "    # Pre-calculate mapping from standard AA to model vocab index. This must use the model alphabet (not a simple 0..19 mapping).\n",
    "    aa_vocab_idx = {aa: alphabet.get_idx(aa) for aa in aa_list}\n",
    "\n",
    "    for i in range(L):\n",
    "        # Mask current position\n",
    "        masked_seq = seq[:i] + \"<mask>\" + seq[i+1:]\n",
    "        data = [(\"seq\", masked_seq)]\n",
    "\n",
    "        # Prepare input (assume batch_converter handles tokenization and padding)\n",
    "        _, _, tokens = batch_converter(data)\n",
    "        tokens = tokens.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # logits shape: (batch=1, L+2, vocab_size). \n",
    "            # Slicing [1:-1] removes BOS and EOS tokens to align with seq.\n",
    "            logits = model(tokens)[\"logits\"][0, 1:-1]\n",
    "\n",
    "        # Locate mask token to extract specific predictions\n",
    "        mask_positions = (tokens[0] == alphabet.mask_idx).nonzero(as_tuple=False).squeeze(-1)\n",
    "        if mask_positions.numel() != 1:\n",
    "            raise ValueError(\n",
    "                f\"Expected exactly one <mask> token, but found {mask_positions.numel()} at position i={i}.\"\n",
    "            )\n",
    "        mask_idx_in_tokens = mask_positions.item()\n",
    "\n",
    "        # Note: tokens include BOS, so mask index needs -1 offset to align with sliced logits\n",
    "        pos_in_logits = mask_idx_in_tokens - 1\n",
    "\n",
    "        # Get vocab log-probabilities at the masked position\n",
    "        log_probs_vocab = rectify_logits(logits[pos_in_logits])  # (vocab_size,)\n",
    "\n",
    "        # Extract log-probabilities for the 20 standard amino acids\n",
    "        for j, aa in enumerate(aa_list):\n",
    "            mut_pref_matrix[i, j] = log_probs_vocab[aa_vocab_idx[aa]].item()\n",
    "\n",
    "    # Convert the 20-AA log-probabilities into a 20-AA conditional distribution per position\n",
    "    llr_tensor = torch.tensor(mut_pref_matrix, dtype=torch.float32)\n",
    "    probs_20aa = F.softmax(llr_tensor, dim=1).cpu().numpy()\n",
    "\n",
    "    # Compute Shannon entropy per position (in bits)\n",
    "    eps = 1e-12\n",
    "    entropy_per_pos = -np.sum(probs_20aa * np.log2(probs_20aa + eps), axis=1)\n",
    "\n",
    "    return entropy_per_pos.tolist()\n",
    "\n",
    "# calculate the sequence entropy of Q8L9G7\n",
    "aa_seq = \"MTVAAGIGYALVALGPSLSLFVSVISRKPFLILTVLSSTLLWLVSLIILSGLWRPFLPLKANVWWPYALLVITSVCFQEGLRFLFWKVYKRLEDVLDSFADRISRPRLFLTDKLQIALAGGLGHGVAHAVFFCLSLLTPAFGPATFYVERCSKVPFFLISAIIALAFVTIHTFSMVIAFEGYAKGNKVDQIIVPVIHLTAGMLTLVNFASEGCVIGVPLLYLVASLTLVHCGKMVWQRLLESRNQSSASR\"\n",
    "entropy_per_pos = compute_masked_entropy(aa_seq)\n",
    "mean_entropy = round(np.mean(entropy_per_pos), 6)\n",
    "\n",
    "print('the sequence entropy is {:.6f}'.format(mean_entropy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
